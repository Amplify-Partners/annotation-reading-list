# Annotation Reading List
A reading list of relevant papers and projects on foundation model annotation

## Consitutional AI and Self-Alignment
- [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047) (May 2023)
- [Constitutional AI: Harmlessness from AI Feedback]([https://arxiv.org/pdf/2212.08073](https://arxiv.org/abs/2212.08073) (Dec 2022)

## Critique
- [LLM Critics Help Catch LLM Bugs](https://arxiv.org/abs/2407.00215) (Jun 2024)
- [CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation](https://arxiv.org/abs/2311.18702) (Nov 2023)
- [Enabling Scalable Oversight via Self-Evolving Critic](https://arxiv.org/abs/2501.05727) (Jan 2025)
- [Self-critiquing models for assisting human evaluators](https://arxiv.org/abs/2206.05802) (Jun 2022)

## Debate
- [AI Safety via Debate](https://arxiv.org/abs/1805.00899) (May 2018)
- [Scalable AI Safety via Doubly-Efficient Debate](https://arxiv.org/abs/2311.14125) (Nov 2023)
- [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782) (Jul 2024)
- [Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://arxiv.org/abs/2305.14325) (May 2023)
- [Scalable AI Safety via Doubly-Efficient Debate](https://arxiv.org/abs/2311.14125) (Nov 2023)
- [Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124) (Jan 2025)

## Iterated Amplification and Weak-to-Strong Generalization
- [Supervising Strong Learners by Amplifying Weak Experts](https://arxiv.org/abs/1810.08575) (Oct 2018)
- [Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning](https://arxiv.org/abs/2402.00667) (Feb 2024)
- [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://arxiv.org/abs/2403.09472) (Mar 2024)
- [Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390) (Dec 2023)
